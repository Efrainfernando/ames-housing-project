{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c548eb8a",
   "metadata": {},
   "source": [
    "# Capítulo 4 · Formulación matricial del modelo OLS\n",
    "\n",
    "> **Objetivo:** derivar y aplicar el estimador matricial de Mínimos Cuadrados Ordinarios (OLS)  \n",
    ">  \n",
    "> $$ \\hat{\\beta} = (X^\\top X)^{-1}X^\\top y $$  \n",
    ">  \n",
    "> usando **las variables candidatas** seleccionadas en el Capítulo 3 y comparar con `statsmodels.OLS`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c481e5",
   "metadata": {},
   "source": [
    "## 4.1 Datos y construcción de \\(X\\) y \\(y\\)\n",
    "\n",
    "En este capítulo construiremos \\(X\\) (predictores) y \\(y\\) (respuesta) a partir del dataset *Ames Housing*.\n",
    "Tomamos como objetivo \\(y = \\texttt{SalePrice}\\) y elegimos **hasta 12 variables candidatas** con alta correlación y baja colinealidad (según el criterio del Cap. 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Localiza el CSV (compatible con tu estructura de proyecto)\n",
    "CANDIDATE_PATHS = [Path('data/ames_housing.csv'), Path('AmesHousing.csv')]\n",
    "for p in CANDIDATE_PATHS:\n",
    "    if p.exists():\n",
    "        DATA_PATH = p\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError('No se encontró data/ames_housing.csv ni AmesHousing.csv')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "target = 'SalePrice'\n",
    "\n",
    "# Asegurar numérico y eliminar infinitos\n",
    "num_df = df.select_dtypes(include=[np.number]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Pool por correlación absoluta con el target\n",
    "corr_abs = num_df.corr(numeric_only=True)[target].dropna().abs().sort_values(ascending=False)\n",
    "\n",
    "# Empezamos con top 15 (excluyendo el propio SalePrice) y filtramos por colinealidad > 0.85\n",
    "pool = [c for c in corr_abs.index if c != target][:15]\n",
    "\n",
    "sel = []\n",
    "for v in pool:\n",
    "    if not sel:\n",
    "        sel.append(v)\n",
    "        continue\n",
    "    ok = True\n",
    "    for u in sel:\n",
    "        r = abs(num_df[[v, u]].dropna().corr().iloc[0,1])\n",
    "        if r > 0.85:\n",
    "            ok = False\n",
    "            break\n",
    "    if ok:\n",
    "        sel.append(v)\n",
    "    if len(sel) >= 12:\n",
    "        break\n",
    "\n",
    "# Construcción de matrices con intercepto\n",
    "data = num_df[[target] + sel].dropna()\n",
    "y = data[target].values.reshape(-1, 1)\n",
    "X = data[sel].values\n",
    "X = np.c_[np.ones((X.shape[0], 1)), X]  # intercepto\n",
    "\n",
    "sel, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a22c3",
   "metadata": {},
   "source": [
    "El vector de variables elegidas `sel` es el conjunto de **candidatas** para este capítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74be89a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2 Derivación del estimador OLS\n",
    "\n",
    "Sea el problema de mínimos cuadrados: minimizar  \n",
    "\n",
    "$$ S(\\beta) = \\|y - X\\beta\\|^2 $$  \n",
    "\n",
    "La condición de primer orden (ecuaciones normales) es  \n",
    "\n",
    "$$ X^\\top X\\,\\hat{\\beta} = X^\\top y $$  \n",
    "\n",
    "Si \\( X^\\top X \\) es invertible (pleno rango), entonces  \n",
    "\n",
    "$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$  \n",
    "\n",
    "> **Invertibilidad:** Se requiere \\( \\operatorname{rango}(X) = p \\) (columnas linealmente independientes).  \n",
    "> Con intercepto, \\( p = \\) número de predictores + 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d4381",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3 Cálculo numérico manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo manual de beta_hat\n",
    "XtX = X.T @ X\n",
    "Xty = X.T @ y\n",
    "\n",
    "# Comprobaciones de rango y número de condición\n",
    "rank = np.linalg.matrix_rank(XtX)\n",
    "cond = np.linalg.cond(XtX)\n",
    "\n",
    "beta_hat = np.linalg.inv(XtX) @ Xty\n",
    "\n",
    "rank, cond, beta_hat[:5].ravel()  # mostramos primeras 5 betas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31053d",
   "metadata": {},
   "source": [
    "### Predicciones, residuos y suma de cuadrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d3f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = X @ beta_hat\n",
    "res  = y - yhat\n",
    "SSR  = float((res.T @ res))  # residual sum of squares\n",
    "n, p = X.shape\n",
    "sigma2_hat = SSR / (n - p)\n",
    "\n",
    "n, p, SSR, sigma2_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422add3",
   "metadata": {},
   "source": [
    "### Varianzas de \\(\\hat{\\beta}\\) y errores estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8597a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Var(beta_hat) = sigma^2 * (X'X)^{-1}\n",
    "var_beta = sigma2_hat * np.linalg.inv(XtX)\n",
    "se_beta = np.sqrt(np.diag(var_beta)).reshape(-1,1)\n",
    "\n",
    "se_beta[:5].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22120ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.4 Comparación con `statsmodels.OLS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Con statsmodels (agrega intercepto)\n",
    "y_sm = data[target].values\n",
    "X_sm = sm.add_constant(data[sel].values, has_constant='add')\n",
    "\n",
    "ols = sm.OLS(y_sm, X_sm).fit()\n",
    "\n",
    "print(ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7286e",
   "metadata": {},
   "source": [
    "> **Coincidencia:** Los coeficientes, errores estándar y métricas (R², SSR) deben coincidir con los obtenidos manualmente (salvo diferencias de redondeo).  \n",
    "> Verifica especialmente `params` y `bse` frente a `beta_hat` y `se_beta` calculados arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9f57a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.5 Discusión: invertibilidad y significado de cada término\n",
    "\n",
    "- \\( X^\\top X \\) **singular** → ocurre con multicolinealidad perfecta (columnas duplicadas o combinación lineal exacta).  \n",
    "- \\( \\kappa(X^\\top X) = \\text{condición} \\) **alta** → problemas de inestabilidad numérica; considerar eliminar predictores redundantes o regularización.  \n",
    "- **Interpretación:**  \n",
    "  - Intercepto: precio esperado cuando los predictores valen cero (interpretar con cautela).  \n",
    "  - Pendientes: cambio esperado en `SalePrice` por unidad del predictor, manteniendo los demás constantes.\n",
    "\n",
    "> Para diagnóstico adicional, considera calcular **VIF** y estandarizar predictores antes del ajuste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246cc30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.6 Key takeaways\n",
    "\n",
    "- El estimador matricial  $$ (X^\\top X)^{-1}X^\\top y $$  coincide con `statsmodels.OLS`.  \n",
    "- La **invertibilidad** depende del rango de \\(X\\); la colinealidad alta puede inflar varianzas y volver inestable la estimación.  \n",
    "- Usar variables **candidatas** reduce colinealidad y mejora interpretabilidad del modelo base que se ampliará en los siguientes capítulos.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}