# Cap√≠tulo 0 ¬∑ Instrucciones de reproducci√≥n y estructura del proyecto

> **Overview:**  
> Este cap√≠tulo presenta las instrucciones para compilar, reproducir y estructurar correctamente el libro del proyecto integrador *‚ÄúInferencia robusta y validaci√≥n en modelos de regresi√≥n lineal usando el Ames Housing Dataset‚Äù*.  
> Se detallan los requerimientos t√©cnicos, control de versiones y lineamientos de formato para todos los cap√≠tulos del libro.

---

## 0.1 C√≥mo compilar el libro

Ejecutar el siguiente comando en la terminal dentro de la carpeta ra√≠z del proyecto:

```bash
jupyter-book build .
```

Los archivos compilados se generan en la carpeta `_build/html/`.

### Dependencias

Instalar las librer√≠as listadas en el archivo `requirements.txt` o alternativamente usar un entorno con `environment.yml`:

```bash
pip install -r requirements.txt
```

---

## 0.2 C√≥mo obtener el dataset

El libro utiliza el conjunto de datos **Ames Housing**.  
Se espera en la ruta:

```
data/ames_housing.csv
```

Puedes descargarlo manualmente o empleando la **Kaggle API**:

```bash
kaggle datasets download -d prevek18/ames-housing-dataset -p data/ --unzip
mv data/AmesHousing.csv data/ames_housing.csv
```

---

## 0.3 Control de versiones

Estructura sugerida del proyecto:

```bash
book/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ ames_housing.csv
‚îú‚îÄ‚îÄ notebooks/
‚îú‚îÄ‚îÄ _build/
‚îú‚îÄ‚îÄ _config.yml
‚îú‚îÄ‚îÄ _toc.yml
‚îî‚îÄ‚îÄ requirements.txt
```

# Cap√≠tulo 1 ¬∑ Demostraciones solicitadas

> **Overview:**  
> En este cap√≠tulo se presentan las demostraciones te√≥ricas fundamentales del modelo de **regresi√≥n lineal simple**, enfocadas en la distribuci√≥n de la suma de cuadrados de los residuos y el origen de los grados de libertad asociados.

---

## 1.1 Modelo de regresi√≥n lineal simple

Sea el modelo

$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad i=1,\ldots,n,
$$

donde los errores cumplen:

$$
\mathbb{E}(\varepsilon_i)=0, \qquad \operatorname{Var}(\varepsilon_i)=\sigma^2.
$$

Definimos los **residuos**:

$$
e_i = y_i - \hat{y}_i = y_i - (\hat{\beta}_0+\hat{\beta}_1 x_i),
$$

y la **suma de cuadrados de los residuos**:

$$
SS_{Res} = \sum_{i=1}^n e_i^2.
$$

---

## 1.2 Objetivo de la demostraci√≥n

Demostrar que:

$$
\frac{SS_{Res}}{\sigma^2} \sim \chi^2_{n-2},
$$

y explicar por qu√© se restan **dos grados de libertad** en el modelo de regresi√≥n simple (asociados a $\hat\beta_0$ y $\hat\beta_1$ ).

---

## 1.3 Marco te√≥rico

En notaci√≥n matricial:

$$
y = X\beta + \varepsilon, \qquad X = 
\begin{bmatrix}
1 & x_1\\
1 & x_2\\
\vdots & \vdots\\
1 & x_n
\end{bmatrix}.
$$

El estimador de m√≠nimos cuadrados ordinarios (MCO) es:

$$
\hat\beta = (X'X)^{-1}X'y,
$$

y las predicciones se obtienen mediante la **matriz sombrero** $H = X(X'X)^{-1}X'$ :

$$
\hat{y} = Hy, \qquad e = y - \hat{y} = (I-H)y.
$$

---

## 1.4 Demostraci√≥n paso a paso

1. **Expresi√≥n de los residuos en funci√≥n de los errores**  

   Sustituyendo $y = X\beta + \varepsilon$ :

   $$
   e = (I-H)(X\beta+\varepsilon) = (I-H)\varepsilon.
   $$

2. **Suma de cuadrados de los residuos**

   $$
   SS_{Res} = e'e = \varepsilon'(I-H)\varepsilon.
   $$

3. **Propiedades clave de la matriz $(I-H)$:**
   - Es **sim√©trica**: $(I-H)' = I-H$,
   - Es **idempotente**: $(I-H)^2 = I-H$,
   - Su rango es $n - p$, donde $p$ es el n√∫mero de par√°metros estimados.

   En el modelo simple, $p = 2$ (intercepto y pendiente), por tanto $\operatorname{rango}(I-H) = n-2$.

4. **Distribuci√≥n del cuadr√°tico:**

   Como $\varepsilon \sim \mathcal N(0,\sigma^2 I)$,

   $$
   \frac{\varepsilon'(I-H)\varepsilon}{\sigma^2} \sim \chi^2_{n-2}.
   $$

   Por lo tanto:

   $$
   \boxed{\displaystyle \frac{SS_{Res}}{\sigma^2} \sim \chi^2_{n-2}}.
   $$

---

## 1.5 Interpretaci√≥n

Los **grados de libertad** reflejan las restricciones impuestas por los par√°metros estimados:
- Cada par√°metro estimado ($\beta_0$, $\beta_1$) ‚Äúconsume‚Äù un grado de libertad.  
- En total se restan 2 grados de libertad al n√∫mero de observaciones $n$.  

Por tanto, la estimaci√≥n insesgada de la varianza del error es:

$$
s^2 = \frac{SS_{Res}}{n-2}.
$$

---

## 1.6 Key takeaways

- $H = X(X'X)^{-1}X'$ es fundamental para expresar los residuos y entender su estructura.  
- $(I-H)$ proyecta los errores sobre el subespacio ortogonal a $X$.  
- El estad√≠stico $\tfrac{SS_{Res}}{\sigma^2}$ sigue una distribuci√≥n $\chi^2_{n-2}$.  
- Los dos grados de libertad perdidos corresponden a los par√°metros estimados.  
- Este resultado sustenta la inferencia sobre $\sigma^2$ y los test t en regresi√≥n.

---

üìö **Referencia:**  
Draper, N. R., & Smith, H. (1998). *Applied Regression Analysis.* Wiley.
